{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "# import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
    "\n",
    "## Planning:\n",
    "The goal is to use value iteration to solve a simple doorkey problem using minigrid as an environment. \n",
    "\n",
    "I want to first build a system to build a good intuition for the value iteration method, then once I have created an agent and training loop based on that upgrade it to a fully connect MLP.\n",
    "\n",
    "## approach\n",
    "I want to first \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] Test the environment with a random policy\n",
    "- [ ] Create a table system for storing states\n",
    "- [ ] Create action selection system based on state estimates\n",
    "- [ ] Create an agent class that implements the actions\n",
    "- [ ] Create a training loop\n",
    "- [ ] Build evaluations\n",
    "- [ ] Rebuild with FCN instead of tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new gym environment with doorkey 5x5 minigrid\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# print gym environment information\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create value iteration table\n",
    "Value iteration is a simple technique where you map each of the possible states that might exist in your environment with an estimation of the value of being in this state.\n",
    "\n",
    "This works because we recursively update our value estimations from our final goal state which gives us maximum reward to infer the value of being in other states. Normally the value of a state is based on whether it moves us closer to or further away from our reward.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our initial table, which is the same size as our environment's state space\n",
    "def initialize_values(env):\n",
    "    return np.zeros(env.observation_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged\n"
     ]
    }
   ],
   "source": [
    "# Calculate q values for each action in a given state\n",
    "\n",
    "def calculate_q_value(env, state, value_table, gamma=0.9):\n",
    "    q_value = []\n",
    "    for action in range(env.action_space.n):\n",
    "        next_states_rewards = []\n",
    "        for next_state_reward in env.P[state][action]:\n",
    "            transition_probability, next_state, reward, _ = next_state_reward\n",
    "            next_states_rewards.append((transition_probability * (reward + gamma * value_table[next_state])))\n",
    "        q_value.append(np.sum(next_states_rewards))\n",
    "\n",
    "    return q_value\n",
    "\n",
    "def update_value_table(value_table, state, q_value):\n",
    "    value_table[state] = max(q_value)\n",
    "    return value_table\n",
    "\n",
    "def has_converged(value_table, updated_value_table, epsilon):\n",
    "    return np.sum(np.fabs(updated_value_table - value_table )) <= epsilon\n",
    "\n",
    "def value_iteration(env, gamma=.99, epsilon=1e-14):\n",
    "    value_table = initialize_values(env)\n",
    "    \n",
    "    while True: \n",
    "        updated_value_table = np.copy(value_table)\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_value = calculate_q_value(env, state, value_table, gamma)\n",
    "            value_table =  update_value_table(value_table, state, q_value)\n",
    "\n",
    "        if has_converged(value_table, updated_value_table, epsilon):\n",
    "            print(\"Value iteration converged\")\n",
    "            break\n",
    "    return value_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_policy(env, value_table, gamma=0.99 ):\n",
    "    observation_space_size = env.observation_space.n\n",
    "    policy = np.zeros(observation_space_size, dtype=np.int64)\n",
    "    for state in range(observation_space_size):\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_state_reward in env.P[state][action]:\n",
    "                trans_prob, next_state, reward_prob, _ = next_state_reward\n",
    "                Q_table[action] +=(trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "        print(np.argmax(Q_table))\n",
    "        policy[state] = int(np.argmax(Q_table))\n",
    "    return policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run episode\n",
    "def run_episode(env, policy):\n",
    "    state, _= env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy[state]\n",
    "        state, reward, done,_,  info = env.step(action)\n",
    "        total_reward += reward \n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=100):\n",
    "    scores = [run_episode(env, policy) for _ in range(num_episodes)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "Average score over 1000 episodes:  0.841\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "num_episodes = 1000\n",
    "value_table = value_iteration(env, gamma=0.99, epsilon=1e-10)\n",
    "policy = extract_policy(env, value_table)\n",
    "score = evaluate_policy(env, policy, num_episodes)\n",
    "print(f\"Average score over {num_episodes} episodes: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_episode_visual(env, policy):\n",
    "    state, _= env.reset()\n",
    "    done = False\n",
    "    frames = []  # List to hold the frames\n",
    "    while not done:\n",
    "        img = env.render()  # Get the current state as an RGB image\n",
    "        frames.append(img)\n",
    "        action = policy[state]\n",
    "        state, reward, done,_ ,  info = env.step(action)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    imageio.mimsave('policy_run.gif', frames, duration=0.1)\n",
    "\n",
    "run_episode_visual(env, policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd21a57e4fd92262eb0452f059baab566dea59e951fa62645d98860e4237e4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
