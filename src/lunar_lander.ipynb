{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# print gym environment information\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
    "\n",
    "## Planning:\n",
    "The goal is to use DQN to solve the lunar lander problem.\n",
    "\n",
    "Currently I am unclear on the following:\n",
    "Exactly how to set up the network to receive inputs and outputs from the system\n",
    "What kind of loss function should be used in this setup to make sure that it is learning correctly.\n",
    "How to account for the various kinds of things can be observed in the observation space.\n",
    "\n",
    "## approach\n",
    "A good first step would be to try and experiment with basic approaches messing around with the system to get a grasp of it.\n",
    "Then I'd like to experiment with a basic neural network feeding in maybe 2 of the parameters.\n",
    "\n",
    "Edit: I think I will try to use the 4 parameters as inputs to the network and see how that goes.\n",
    "Playing with the network structure and the loss function should be the next step.\n",
    "Messing around trying to solve it by hand will probably not be very useful.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [] Create agent class that can interact with the environment\n",
    "- [] Add the neural network to the actions of the agent\n",
    "- [] Add the loss function to the agent\n",
    "- [] Add the replay buffer to the agent\n",
    "- [] Add the epsilon greedy policy to the agent\n",
    "- [] Add the training loop to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self, n_observations, fc1_dims=128, fc2_dims=128, n_actions=4):\n",
    "        super(network, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_observations, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "\n",
    " \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = T.softmax(self.pi(x), dim=1)\n",
    "\n",
    "        return pi\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class replay_memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self) -> None:\n",
    "        actor_dims = env.observation_space.shape[0]\n",
    "        n_actions = env.action_space.n\n",
    "        self.policy_network =  network(input_dims=actor_dims, n_actions=n_actions)\n",
    "        self.target_network = network(input_dims=actor_dims, n_actions=n_actions)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.memory = replay_memory(100000)\n",
    "        self.steps_done = 0\n",
    "        self.batch_size = 128\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 1000\n",
    "        self.tau = 0.005\n",
    "        self.LR = 0.001\n",
    "\n",
    "        self.max_score = -math.inf\n",
    "        self.min_score = math.inf\n",
    "        self.avg_score = 0\n",
    "\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.LR, amsgrad=True)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.chkpt_file = 'tmp/lunar_lander'\n",
    "\n",
    "\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.network.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "        \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\ \n",
    "            math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.network.device)\n",
    "        probabilities = self.network.forward(state)\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        log_probs = action_probs.log_prob(action)\n",
    "        self.memory.append((log_probs))\n",
    "\n",
    "        return action.item()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (pi): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# take action\u001b[39;00m\n\u001b[1;32m     15\u001b[0m observation, state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> 16\u001b[0m result \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mforward(T\u001b[39m.\u001b[39;49mtensor(state)\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     18\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m, in \u001b[0;36mnetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(state))\n\u001b[1;32m     16\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     17\u001b[0m     pi \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi(x), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/werk/RL_fundamentals/rl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/werk/RL_fundamentals/rl_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "\n",
    "# reset environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "images = []\n",
    "count = 0\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    count +=1\n",
    "    # render environment\n",
    "    image = env.render()\n",
    "    images.append(image)\n",
    "    # sample random action\n",
    "    action = env.action_space.sample()\n",
    "    # take action\n",
    "    observation, state, reward, done, info = env.step(action)\n",
    "    state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "    result = agent.forward(T.tensor(state).float())\n",
    "\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd21a57e4fd92262eb0452f059baab566dea59e951fa62645d98860e4237e4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
