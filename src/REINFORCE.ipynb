{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "import imageio\n",
    "\n",
    "\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
    "\n",
    "## Planning:\n",
    "The goal is to build a simple policy gradient algorithm (AKA REINFORCE), as a way of getting more comfortable with this approach. This kind of technique is used frequently in other architectures such as PPO, DDPG and a2c.\n",
    "\n",
    "## approach\n",
    "\n",
    "My approach here is to first create a simple policy network which can take in states as input and output a distribution over the possible actions.\n",
    "\n",
    "I will then set up an agent which uses the policy network to make decisions, and can train that policy by calculating loss based on the reward from the environment.\n",
    "\n",
    "I will then setup a training loop where the network can take actions and update its policy.\n",
    "\n",
    "After\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [*] Create basic network that represents our policy.\n",
    "- [*] Create an agent that chooses an action based on what it knows about the environment\n",
    "- [*] Add functions to the agent allowing it to calculate loss based on its action and update its policy\n",
    "- [*] Build a training loop and have our agent train.\n",
    "- [] Train and produce graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
      "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
      "  1.         1.       ], (8,), float32)\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# print gym environment information\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5\n",
    "    # render_mode=\"rgb_array\"\n",
    ")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(FCN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, fc1_dims)\n",
    "        self.layer2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.layer3 = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        logits = self.layer3(x)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "# Produces distribution of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2369, 0.2524, 0.2708, 0.2399], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = FCN(env.observation_space.shape[0], env.action_space.n)\n",
    "state, info = env.reset()\n",
    "state = torch.Tensor(state)\n",
    "network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, env, lr):\n",
    "        self.network = FCN(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=lr)\n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.log_probs = None\n",
    "        self.env = env\n",
    "    \n",
    "    def get_distribution(self, state):\n",
    "        distribution = self.network(state)\n",
    "        distribution = torch.distributions.Categorical(distribution)\n",
    "        return distribution\n",
    "\n",
    "    def train(self,  gamma, num_episodes):\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            self.rewards = []\n",
    "            self.log_probs = []\n",
    "            state, info = env.reset()\n",
    "            terminated = False\n",
    "\n",
    "\n",
    "            while not terminated:\n",
    "                state = torch.from_numpy(state).float()\n",
    "            \n",
    "                distribution = self.get_distribution(state)\n",
    "                action = distribution.sample()\n",
    "                log_prob = distribution.log_prob(action)\n",
    "                self.log_probs.append(log_prob)\n",
    "\n",
    "                state, rewards, terminated, _, _ = env.step(action.item())\n",
    "                \n",
    "                self.rewards.append(rewards)\n",
    "            \n",
    "            self.update_policy(gamma)\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Completed episode {episode} and achieved score {sum(self.rewards)}\")\n",
    "    \n",
    "    def update_policy(self, gamma):\n",
    "        total_future_rewards = []\n",
    "        total_future_reward = 0\n",
    "\n",
    "        for reward in self.rewards:\n",
    "            total_future_reward = total_future_reward * gamma + reward\n",
    "            total_future_rewards.append(total_future_reward)\n",
    "        \n",
    "        returns = total_future_rewards[::-1]\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, total_future_reward in zip(self.log_probs, returns):\n",
    "            loss = -log_prob * total_future_reward\n",
    "            loss = loss.unsqueeze(0)\n",
    "            policy_loss.append(loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n",
      "Completed episode 0 and achieved score 23.0\n",
      "Completed episode 50 and achieved score 56.0\n",
      "Completed episode 100 and achieved score 56.0\n",
      "Completed episode 150 and achieved score 40.0\n",
      "Completed episode 200 and achieved score 115.0\n",
      "Completed episode 250 and achieved score 94.0\n",
      "Completed episode 300 and achieved score 48.0\n",
      "Completed episode 350 and achieved score 194.0\n",
      "Completed episode 400 and achieved score 372.0\n",
      "Completed episode 450 and achieved score 168.0\n",
      "Completed episode 500 and achieved score 108.0\n",
      "Completed episode 550 and achieved score 123.0\n",
      "Completed episode 600 and achieved score 376.0\n",
      "Completed episode 650 and achieved score 191.0\n",
      "Completed episode 700 and achieved score 348.0\n",
      "Completed episode 750 and achieved score 193.0\n",
      "Completed episode 800 and achieved score 412.0\n",
      "Completed episode 850 and achieved score 221.0\n",
      "Completed episode 900 and achieved score 238.0\n",
      "Completed episode 950 and achieved score 135.0\n",
      "Completed episode 1000 and achieved score 118.0\n",
      "Completed episode 1050 and achieved score 154.0\n",
      "Completed episode 1100 and achieved score 112.0\n",
      "Completed episode 1150 and achieved score 249.0\n",
      "Completed episode 1200 and achieved score 401.0\n",
      "Completed episode 1250 and achieved score 531.0\n",
      "Completed episode 1300 and achieved score 714.0\n",
      "Completed episode 1350 and achieved score 2213.0\n",
      "Completed episode 1400 and achieved score 810.0\n",
      "Completed episode 1450 and achieved score 5870.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3429/3986965878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# agent.run_episode_and_save_gif(env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mguy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3429/1907321903.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gamma, num_episodes)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completed episode {episode} and achieved score {sum(self.rewards)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3429/1907321903.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, gamma)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print gym environment information\n",
    "env = gym.make(\n",
    "    \"CartPole-v1\"\n",
    "    # continuous = False,\n",
    "    # gravity = -10.0,\n",
    "    # enable_wind = False,\n",
    "    # wind_power = 15.0,\n",
    "    # turbulence_power = 1.5\n",
    "    # render_mode=\"rgb_array\"\n",
    ")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "guy = agent(env, lr=0.0001)\n",
    "\n",
    "# agent.run_episode_and_save_gif(env)\n",
    "guy.train( gamma=0.99, num_episodes=3000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence and great success with cartpole\n",
    "We note here that REINFORCE does converge with cartpole, and not only that but massively outperforms what we could achieve with DQN. \n",
    "\n",
    "I speculate that this may be because REINFORCE is able to more precisely model the correct actions within the narrow range available to cartpole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-90.        -90.         -5.         -5.         -3.1415927  -5.\n",
      "  -0.         -0.       ], [90.        90.         5.         5.         3.1415927  5.\n",
      "  1.         1.       ], (8,), float32)\n",
      "action space: Discrete(4)\n",
      "Completed episode 0 and achieved score -288.1393010147376\n",
      "Completed episode 50 and achieved score -121.08381794907808\n",
      "Completed episode 100 and achieved score -93.2132013565292\n",
      "Completed episode 150 and achieved score -124.9879530865401\n",
      "Completed episode 200 and achieved score -159.7268319041226\n",
      "Completed episode 250 and achieved score -189.42891953151076\n",
      "Completed episode 300 and achieved score -119.91641968438985\n",
      "Completed episode 350 and achieved score -114.7540347145775\n",
      "Completed episode 400 and achieved score -119.1686646292804\n",
      "Completed episode 450 and achieved score -65.50800505743473\n",
      "Completed episode 500 and achieved score -278.30043274229365\n",
      "Completed episode 550 and achieved score -10.612656103722045\n",
      "Completed episode 600 and achieved score -137.621362467181\n",
      "Completed episode 650 and achieved score -185.02167351463828\n",
      "Completed episode 700 and achieved score -335.79624369690583\n",
      "Completed episode 750 and achieved score -179.0168126645142\n",
      "Completed episode 800 and achieved score -76.28747067276598\n",
      "Completed episode 850 and achieved score -289.8213916184219\n",
      "Completed episode 900 and achieved score -246.35650892190037\n",
      "Completed episode 950 and achieved score -158.88597831770073\n",
      "Completed episode 1000 and achieved score -438.02912224839986\n",
      "Completed episode 1050 and achieved score -395.0462208464395\n",
      "Completed episode 1100 and achieved score -119.41272553895574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3429/2382416591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# agent.run_episode_and_save_gif(env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mguy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3429/1907321903.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gamma, num_episodes)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print gym environment information\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5\n",
    "    # render_mode=\"rgb_array\"\n",
    ")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "guy = agent(env, lr=0.00003)\n",
    "\n",
    "# agent.run_episode_and_save_gif(env)\n",
    "guy.train( gamma=0.99, num_episodes=3000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence fails on lunar lander\n",
    "We can see that the REINFORCE algorithm with the parameters struggles to converge for lunar lander.\n",
    "This could be because the observation space is too large for this algorithm to converge. \n",
    "\n",
    "It could also be the adjustments we are making to our network are too large and cause the training to be too unstable. We do see better performance when lower learning rates are applied.\n",
    "\n",
    "An experiment I would want to try would be to test the continuous version of this environment and see if REINFORCE does better there, due to the nature of its policy adjustments. \n",
    "\n",
    "For now however I want to move on to PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd21a57e4fd92262eb0452f059baab566dea59e951fa62645d98860e4237e4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
