{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "import imageio\n",
    "\n",
    "\n",
    "import torch as T\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque , namedtuple\n",
    "from itertools import count\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning:\n",
    "\n",
    "In this notebook I am building an implementation of the PPO algorithm.\n",
    "\n",
    "I believe that this is quite similar to the REINFORCE algorithm, so the first thing I want to do is understand the key differences so that I can quickly put a working version of the algorithm together to solve Lunar Lander.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [*] Build basic version of the pieces of PPO.\n",
    "- [*] Implement the basic FCN that represents our policy\n",
    "- [*] Create the Agent class\n",
    "- [] Collect the trajectories of the agent through each episode\n",
    "- [] Add policy updates to the agent class\n",
    "- [] Add training loop and test ability to learn\n",
    "- [] Implement minibatch updates\n",
    "- [] Implement multiple epochs over the same data\n",
    "- [] Implement concurrent training of the value function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# print gym environment information\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5\n",
    "    # render_mode=\"rgb_array\"\n",
    ")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(FCN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, fc1_dims)\n",
    "        self.layer2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.layer3 = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        logits = self.layer3(x)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "# Produces distribution of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_observations, n_actions, env, gamma=0.99, lr=0.001, clip_epsilon=0.2, gae_lambda =0.95 ):\n",
    "        self.policy = FCN(n_observations, n_actions).to(device)\n",
    "        self.value_network = FCN(n_observations, 1).to(device)\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.env = env\n",
    "\n",
    "    def choose_act(self, state):\n",
    "        \"\"\"This function samples the distribution of actions from the policy.\n",
    "        It then returns a distinct action from the distribution\"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        state_tensor = state_tensor.unsqueeze(0)\n",
    "\n",
    "        distribution = self.policy(state_tensor)\n",
    "        action_selection = torch.multinomial(distribution, num_samples=1)\n",
    "        return action_selection\n",
    "    \n",
    "    def collect_trajectories(self, num_steps= 2000):\n",
    "        \n",
    "        rewards, actions, distributions, next_states, dones, states = [], [], [], [], [], []\n",
    "\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            action, distribution = self.choose_act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            distributions.append(distribution)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            states.append(state)\n",
    "\n",
    "            if done:\n",
    "                state, info = env.reset()\n",
    "                \n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "            \n",
    "        states = torch.tensor(states).float().to(device)\n",
    "        actions = torch.tensor(actions).to(device)\n",
    "        rewards = torch.tensor(rewards).float().to(device)\n",
    "        next_states = torch.tensor(next_states).float().to(device)\n",
    "        dones = torch.tensor(dones).float().to(device)\n",
    "        distributions = torch.tensor(distributions).float().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, distributions\n",
    "    \n",
    "    def calculate_advantanges(self, rewards, states, next_states, dones):\n",
    "        \n",
    "        values = self.value_network(states).detach().squeeze()\n",
    "        next_values = self.value_network(next_states).detach().squeeze()\n",
    "\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae = torch.tensor(0.0).to(device)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        future_return = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        for t in reversed(range(len(rewards) -1)):\n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1 -dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1-dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "            future_return = rewards[t] + self.gamma * (1 - dones[t]) * future_return\n",
    "            returns[t] = future_return\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update_policy(self, returns, advantages):\n",
    "\n",
    "\n",
    "    def train(self, epochs, mini_batch_size):        \n",
    "        # here we will need to calculate advantage for our particular run, as well as set up our training loop.\n",
    "        # For each episode we want it to collect trajectories, and then calculate advantage for that particular trajectory.\n",
    "        # then once we have collected the trajectory we'll want to apply it using backprop and autograd.\n",
    "        for i in range(epochs):\n",
    "            states, actions, rewards, next_states, dones, distributions = self.collect_trajectories(num_episodes = 2000)\n",
    "            advantages, returns = self.calculate_advantanges(rewards, states, next_states, dones)\n",
    "\n",
    "\n",
    "            dataset = list(zip(states, actions,advantages, returns))\n",
    "            np.random.shuffle(dataset)\n",
    "\n",
    "            for batch_num in range(0, len(dataset), mini_batch_size):\n",
    "                mini_batch = dataset[batch_num:batch_num + mini_batch_size]\n",
    "                states_batch, actions_batch, advantages_batch, returns_batch = zip(*mini_batch)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.6557198e-03,  1.3889743e+00,  1.7849378e-01, -5.0053096e-01,\n",
       "        -2.1347743e-03,  5.1997055e-04,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32),\n",
       " -0.9181189062574606,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "Agey = PPOAgent(o,actions)\n",
    "state, info = env.reset()\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "action = Agey.choose_act(state)\n",
    "env.step(int(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment(env):\n",
    "    state, _ = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done,truncated, _ = env.step(action)\n",
    "    print(type(state))\n",
    "    assert isinstance(state, np.ndarray), \"State is not a numpy array\"\n",
    "    assert isinstance(reward, (int, float)), \"Reward is not a scalar\"\n",
    "    assert isinstance(done, bool), \"Done is not a boolean\"\n",
    "\n",
    "    print(\"Environment test passed!\")\n",
    "\n",
    "def test_act(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    action = agent.act(state)\n",
    "    assert 0 <= action <= env.action_space.n, \"Action not in action space\"\n",
    "\n",
    "    print(\"`act` method test passed!\")\n",
    "\n",
    "def test_value_network_output(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    value = agent.value_network(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "\n",
    "    assert value.size(0) == 1, \"Value network output isn't a scalar for a single state\"\n",
    "\n",
    "    print(\"Value network output test passed!\")\n",
    "\n",
    "\n",
    "def test_advantage_calculation(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    rewards, values = [], []\n",
    "\n",
    "    for _ in range(10):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        value = agent.value_network(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "\n",
    "    advantages = agent.estimate_advantage(rewards, values)  # replace with your method name\n",
    "\n",
    "    assert len(advantages) == 10, \"Advantage calculation length mismatch\"\n",
    "\n",
    "    print(\"Advantage calculation test passed!\")\n",
    "\n",
    "def test_policy_update(agent):\n",
    "    initial_weights = torch.cat([p.flatten() for p in agent.policy.parameters()])\n",
    "    \n",
    "    # Collect some data here (states, actions, rewards, ...)\n",
    "    # and then update the policy\n",
    "    agent.policy_update()  # replace with your method name\n",
    "\n",
    "    updated_weights = torch.cat([p.flatten() for p in agent.policy.parameters()])\n",
    "    assert not torch.equal(initial_weights, updated_weights), \"Policy weights did not change after update\"\n",
    "\n",
    "    print(\"Policy update test passed!\")\n",
    "\n",
    "def test_policy_gradient(agent, env):\n",
    "    state,_ = env.reset()\n",
    "    action_probabilities = agent.policy(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "    chosen_action_prob = action_probabilities[0, env.action_space.sample()]  # Sample an action to mimic choosing\n",
    "    chosen_action_prob.backward()\n",
    "    for param in agent.policy.parameters():\n",
    "        assert param.grad is not None, f\"Gradient not set for {param}\"\n",
    "    print(\"Policy gradient test passed!\")\n",
    "\n",
    "def test_value_gradient(agent, env):\n",
    "    state, _ = env.reset()\n",
    "    value_estimate = agent.value_network(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "    value_loss = (value_estimate - torch.tensor([1.0], device=device))**2  # Dummy target value\n",
    "    value_loss.backward()\n",
    "    for param in agent.value_network.parameters():\n",
    "        assert param.grad is not None, f\"Gradient not set for {param}\"\n",
    "    print(\"Value gradient test passed!\")\n",
    "\n",
    "def test_gae_calculation(agent, env):\n",
    "    states, rewards, dones, values = [], [], [], []\n",
    "    state, info = env.reset()\n",
    "    for _ in range(10):\n",
    "        values.append(agent.value_network(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)).item())\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    advantages, _ = agent.compute_gae(rewards, dones, values, values)  # Assuming you have this method or similar\n",
    "\n",
    "    assert len(advantages) == 10, \"GAE calculation length mismatch\"\n",
    "\n",
    "    print(\"GAE calculation test passed!\")\n",
    "\n",
    "def test_clipped_objective(agent, env):\n",
    "    old_probs, new_probs, advantages = [], [], []\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(10):\n",
    "        action_probabilities = agent.policy(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "        action = agent.act(state)\n",
    "        old_probs.append(action_probabilities[0, action].item())\n",
    "        state, _, _, _, _ = env.step(action)\n",
    "        action_probabilities = agent.policy(torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0))\n",
    "        new_probs.append(action_probabilities[0, action].item())\n",
    "        advantages.append(1.0)  # dummy advantage\n",
    "\n",
    "    surrogate_objective = agent.compute_surrogate_objective(old_probs, new_probs, advantages)  # You might have a different method name\n",
    "\n",
    "    assert surrogate_objective <= (1 + agent.clip_epsilon) * advantages[0], \"Objective exceeds upper bound\"\n",
    "    assert surrogate_objective >= (1 - agent.clip_epsilon) * advantages[0], \"Objective falls below lower bound\"\n",
    "\n",
    "    print(\"Clipped objective test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Environment test passed!\n",
      "`act` method test passed!\n",
      "Value network output test passed!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor([[2]]) (<class 'torch.Tensor'>) invalid ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m test_act(Agey, env)\n\u001b[1;32m      6\u001b[0m test_value_network_output(agent\u001b[39m=\u001b[39mAgey, env\u001b[39m=\u001b[39menv)\n\u001b[0;32m----> 7\u001b[0m test_advantage_calculation(agent\u001b[39m=\u001b[39;49mAgey, env\u001b[39m=\u001b[39;49menv)\n\u001b[1;32m      8\u001b[0m test_policy_update(agent\u001b[39m=\u001b[39mAgey)\n\u001b[1;32m      9\u001b[0m test_policy_gradient(agent\u001b[39m=\u001b[39mAgey, env\u001b[39m=\u001b[39menv)\n",
      "Cell \u001b[0;32mIn[48], line 34\u001b[0m, in \u001b[0;36mtest_advantage_calculation\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m     33\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n\u001b[0;32m---> 34\u001b[0m     state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     35\u001b[0m     value \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mvalue_network(torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     36\u001b[0m     rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/minigrid/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py:524\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    522\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(action, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(\n\u001b[1;32m    525\u001b[0m         action\n\u001b[1;32m    526\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m \u001b[39m# Apply Engine Impulses\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[39m# Tip is a the (X and Y) components of the rotation of the lander.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m tip \u001b[39m=\u001b[39m (math\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle), math\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle))\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor([[2]]) (<class 'torch.Tensor'>) invalid "
     ]
    }
   ],
   "source": [
    "o = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "Agey = PPOAgent(o,actions)\n",
    "test_environment(env)\n",
    "test_act(Agey, env)\n",
    "test_value_network_output(agent=Agey, env=env)\n",
    "test_advantage_calculation(agent=Agey, env=env)\n",
    "test_policy_update(agent=Agey)\n",
    "test_policy_gradient(agent=Agey, env=env)\n",
    "test_value_gradient(agent=Agey, env=env)\n",
    "test_gae_calculation(agent=Agey, env=env)\n",
    "test_clipped_objective(agent=Agey, env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd21a57e4fd92262eb0452f059baab566dea59e951fa62645d98860e4237e4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
